{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Assignment 2\n",
    "## Advanced Financial Economics M-335\n",
    "\n",
    "#### An Exercise of Financial Crises Prediction Using Machine Learning Techniques.\n",
    "\n",
    "This assignment is inspired by the research article *'Credit growth, the yield curve and financial\n",
    "crisis prediction: evidence from a machine\n",
    "learning approach'* (2019), by Bluwstein et al. The idea is to reproduce the financial crises prediction exercise in that paper on a smaller scale.\n",
    "\n",
    "Our prediction exercise takes the form of a binary classification problem, where each datapoint characterized by a vector of predictors $\\textbf{x}_t=(x^1_t, x^2_t,\\dots,x^N_t)$ realized at time $t$ must be classified into one of two categories: 1) there **will** be a financial crises **at time *t+1 or t+2***. 2) there **will not** be a financial crises **at time *t+1 or t+2***. Time *t* is measured in years. Your task is to implement this prediction problem using five machine learning algorithms:\n",
    " 1. logistic regression\n",
    " 2. logistic regression with LASSO regularization\n",
    " 3. random trees\n",
    " 4. random forest\n",
    " 5. neural networks. \n",
    "   \n",
    "Your task is to assess the accuracy of each and make all the performance comparisons you deem appropriate from what we have learned in class.\n",
    "\n",
    "I will walk you through the beginning of this exercise, basically importing and cleaning the data. Then I'll leave you to the rest...\n",
    "\n",
    "To get started, open the Jupyter Notebook where you will execute your assignment."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "##### 1. Import the Python modules you need\n",
    "First import the modules you need (you will certainly need to import more later on, `matplotlib` or `seaborn` for plotting, `sklearn` indeed. By the way, remember to add `%matplotlib inline` to show plots)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 2. Load the dataset as a Pandas Data Frame\n",
    "Import the dataset from the Excel file (located in the same directory as notebook) as a Pandas DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "df=pd.read_excel(\"JSTdatasetR4.xlsx\",sheet_name=\"Data\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 3. Creating the desired variables in a new Data Frame\n",
    "Now we need to do a few manipulations on the database. Basically we need to create a smaller dataframe comprising only the variables we need, which are those in the *Baseline Experiment* of Bluwstein et al (2019). The code below (with comments) reports how I would do it, but feel free to follow your own method based on your reading of the paper:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['Australia', 'Belgium', 'Canada', 'Denmark', 'Finland', 'France',\n",
       "       'Germany', 'Italy', 'Japan', 'Netherlands', 'Norway', 'Portugal',\n",
       "       'Spain', 'Sweden', 'Switzerland', 'UK', 'USA'], dtype=object)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.country.unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#let's make a copy, in order to preserve original dataset\n",
    "df_copy=df.copy()\n",
    "#let's create new (temporary) columns with the transormed variables we need:\n",
    "#-slope of the yield curve\n",
    "df_copy[\"slope_yield_curve\"]=df_copy[\"ltrate\"]/100-df_copy[\"stir\"]/100\n",
    "# credit: loans to the privete sector / gdp\n",
    "df_copy[\"credit\"]=df_copy[\"tloans\"]/df_copy[\"gdp\"]\n",
    "# debt service ratio: credit * long term interest rate\n",
    "df_copy[\"debt_serv_ratio\"]=(df_copy[\"tloans\"]/df_copy[\"gdp\"])*df_copy[\"ltrate\"]/100\n",
    "# broad money over gdp\n",
    "df_copy[\"bmoney_gdp\"]=df_copy[\"money\"]/df_copy[\"gdp\"]\n",
    "# current account over gdp\n",
    "df_copy[\"curr_acc_gdp\"]=df_copy[\"ca\"]/df_copy[\"gdp\"]\n",
    "# Now we need to compute 1-year absolute variations and percentage variations for a few variables\n",
    "# Obviosly this must be done country-wise, so we cannot act on the dataframe as it is.\n",
    "# a Convenient way of doing this is the Pandas method 'groupby()'\n",
    "df_copy_group=df_copy.groupby(\"iso\") # 'iso' is the country code\n",
    "# create 1 year-variation of credit from grouped dataframe and add back to initial dataframe\n",
    "df_copy[\"delta_credit\"]=df_copy_group[\"credit\"].diff(periods=1)\n",
    "# create 1 year-variation of debt ser ratio from grouped dataframe and add back to initial dataframe\n",
    "df_copy[\"delta_debt_serv_ratio\"]=df_copy_group[\"debt_serv_ratio\"].diff(periods=1)\n",
    "# create 1 year-variation of investment/gdp from grouped dataframe and add back to initial dataframe\n",
    "df_copy[\"delta_investm_ratio\"]=df_copy_group[\"iy\"].diff(periods=1)\n",
    "# create 1 year-variation of public debt/gdp from grouped dataframe and add back to initial dataframe\n",
    "df_copy[\"delta_pdebt_ratio\"]=df_copy_group[\"debtgdp\"].diff(periods=1)\n",
    "# create 1 year-variation of broad money / gdp from grouped dataframe and add back to initial dataframe\n",
    "df_copy[\"delta_bmoney_gdp\"]=df_copy_group[\"bmoney_gdp\"].diff(periods=1)\n",
    "# create 1 year-variation of current / gdp from grouped dataframe and add back to initial dataframe\n",
    "df_copy[\"delta_curr_acc_gdp\"]=df_copy_group[\"curr_acc_gdp\"].diff(periods=1)\n",
    "# now we need to create new variables which are 1-year growth rates of existing ones\n",
    "\n",
    "# we will need this function to apply to the columns of the dataframe\n",
    "\n",
    "def lag_pct_change(x):\n",
    "    \"\"\" Computes percentage changes \"\"\"\n",
    "    lag = np.array(pd.Series(x).shift(1))\n",
    "    return (x - lag) / lag\n",
    "\n",
    "# create 1 year growth rate of CPI from grouped dataframe and add back to initial dataframe\n",
    "df_copy[\"growth_cpi\"]=df_copy_group[\"cpi\"].apply(lag_pct_change)\n",
    "# create 1 year growth rate of consumption per capita from grouped dataframe and add back to initial dataframe\n",
    "df_copy[\"growth_cons\"]=df_copy_group[\"rconpc\"].apply(lag_pct_change)\n",
    "\n",
    "# low let's create the crises early warning label: a dummy variable which takes value one if in the next \n",
    "# or two there will be a crises\n",
    "\n",
    "# temporary array of zeros, dimension number of rows in database\n",
    "temp_array=np.zeros(len(df_copy))\n",
    "# loop to create dummy\n",
    "for i in np.arange(0,len(df_copy)-2):\n",
    "    temp_array[i]= 1 if ( (df_copy.loc[i+1,'crisisJST']== 1) or (df_copy.loc[i+2,'crisisJST']== 1)  ) else 0\n",
    "\n",
    "#put the dummy in the dataframe\n",
    "\n",
    "df_copy[\"crisis_warning\"]=temp_array.astype(\"int64\")\n",
    "\n",
    "# create a smaller dataframe including only the variables we are interested in: the first ten are predictors (X) and the last one is the output, or label (y)\n",
    "variables=[\"slope_yield_curve\",\"delta_credit\",\"delta_debt_serv_ratio\",\"delta_investm_ratio\",\"delta_pdebt_ratio\",\"delta_bmoney_gdp\",\"delta_curr_acc_gdp\",\"growth_cpi\",\"growth_cons\",\"eq_tr\",\"crisis_warning\"]\n",
    "df_final=df_copy[variables].dropna()\n",
    "\n",
    "# let's also create a version of our dataframe which includes the year\n",
    "df_final_withyear=df_copy[[\"year\"]+variables].dropna()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice that Bluwstein et al (2019) drop more observations than what I did in order to obtain more robust results. Fill free to follow their procedure more closely. Otherwise, I am fine with `df_final`.\n",
    "\n",
    "##### 4. Now perform your analysis\n",
    "Remember that the feature that you need to predict (the outcome $y$) is the variable `df_final[\"crisis_warning\"]`, while all the other columns in the data frame are the features $x$ that you use to predict it.\n",
    "\n",
    "Inspired by what we have learned from the notebooks `regression.ipynb` and `classification.ipynb`, and possibly your reading of the additional material in the repository, perform your data analysis:\n",
    "\n",
    "1. Randomly split the data into a training and a test sample.\n",
    "2. Fit the following models on the training sample:\n",
    "    * logistic regression\n",
    "    * logistic regression with LASSO regularization. Here, select the regularization parameter using a 5-fold cross validation\n",
    "    *  random trees. Experiment with different tree depths, not necessarily with a cross validation\n",
    "    *  random forest \n",
    "    *  neural networks. Experiment with different numbers of hidden layers, and neurons for each layers, not necessarily using a cross-validation\n",
    "3. Plot the ROC curves for the best versions of your models and compute the AUROC. According to this criterion, which model performs best ?\n",
    "4. Compare the confusion matrices generated by the models.\n",
    "5. Which variables do 'survive' in the logistic regression with LASSO ? \n",
    "6. OPTIONAL: Is there a way in the logistic regression to conclude which variables are more important for the prediction performance ?\n",
    "7. OPTIONAL: Now let's see if in a real-time experiment any of our models would have predicted the financial crises of 2007-08. Put all the observations before (and including) 2005 in the training sample, and the rest in the test sample. You can use the data frame `df_final_withyear` for this purpose. Fit your preferred model for each of the 5 categories on the training sample. Would have they warned us in 2006 and 2007 of the imminent financial crises?  (for the logistic regressions, in order to draw this conclusion use the probability thresholds which, on  the ROC curves, obtain an 80\\% rate of true positives).\n",
    "8. OPTIONAL: compute any indicator you like......  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import roc_auc_score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Train test split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df_final.drop('crisis_warning', axis = 1)\n",
    "y = df_final.crisis_warning\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.2, random_state = 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Fitting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5185185185185185"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "log_reg = LogisticRegression(penalty = 'none', max_iter = 1000).fit(X_train, y_train)\n",
    "roc_auc_score(y_test, log_reg.predict(X_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Logistic Regression with Lasso"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "log_reg_lasso = LogisticRegression(penalty = 'l1', solver = 'liblinear', max_iter = 1000)\n",
    "parameters = {'C':[1, 10, 100]}\n",
    "grid_search_results = GridSearchCV(log_reg_lasso, parameters).fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-25.82032589,   6.54519306,  64.88130481,  15.381356  ,\n",
       "         -3.9211358 ,   0.87129992,  -5.05051519,  -5.98374812,\n",
       "         -6.2618073 ,   0.23731307]])"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "log_reg.coef_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>mean_fit_time</th>\n",
       "      <th>std_fit_time</th>\n",
       "      <th>mean_score_time</th>\n",
       "      <th>std_score_time</th>\n",
       "      <th>param_C</th>\n",
       "      <th>params</th>\n",
       "      <th>split0_test_score</th>\n",
       "      <th>split1_test_score</th>\n",
       "      <th>split2_test_score</th>\n",
       "      <th>split3_test_score</th>\n",
       "      <th>split4_test_score</th>\n",
       "      <th>mean_test_score</th>\n",
       "      <th>std_test_score</th>\n",
       "      <th>rank_test_score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.007026</td>\n",
       "      <td>0.001722</td>\n",
       "      <td>0.002756</td>\n",
       "      <td>0.000990</td>\n",
       "      <td>1</td>\n",
       "      <td>{'C': 1}</td>\n",
       "      <td>0.933086</td>\n",
       "      <td>0.933086</td>\n",
       "      <td>0.929368</td>\n",
       "      <td>0.929368</td>\n",
       "      <td>0.932836</td>\n",
       "      <td>0.931549</td>\n",
       "      <td>0.001783</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.004653</td>\n",
       "      <td>0.002219</td>\n",
       "      <td>0.001770</td>\n",
       "      <td>0.000415</td>\n",
       "      <td>10</td>\n",
       "      <td>{'C': 10}</td>\n",
       "      <td>0.933086</td>\n",
       "      <td>0.933086</td>\n",
       "      <td>0.929368</td>\n",
       "      <td>0.929368</td>\n",
       "      <td>0.932836</td>\n",
       "      <td>0.931549</td>\n",
       "      <td>0.001783</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.003517</td>\n",
       "      <td>0.000396</td>\n",
       "      <td>0.001769</td>\n",
       "      <td>0.000601</td>\n",
       "      <td>100</td>\n",
       "      <td>{'C': 100}</td>\n",
       "      <td>0.929368</td>\n",
       "      <td>0.929368</td>\n",
       "      <td>0.929368</td>\n",
       "      <td>0.929368</td>\n",
       "      <td>0.929104</td>\n",
       "      <td>0.929315</td>\n",
       "      <td>0.000105</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   mean_fit_time  std_fit_time  mean_score_time  std_score_time param_C  \\\n",
       "0       0.007026      0.001722         0.002756        0.000990       1   \n",
       "1       0.004653      0.002219         0.001770        0.000415      10   \n",
       "2       0.003517      0.000396         0.001769        0.000601     100   \n",
       "\n",
       "       params  split0_test_score  split1_test_score  split2_test_score  \\\n",
       "0    {'C': 1}           0.933086           0.933086           0.929368   \n",
       "1   {'C': 10}           0.933086           0.933086           0.929368   \n",
       "2  {'C': 100}           0.929368           0.929368           0.929368   \n",
       "\n",
       "   split3_test_score  split4_test_score  mean_test_score  std_test_score  \\\n",
       "0           0.929368           0.932836         0.931549        0.001783   \n",
       "1           0.929368           0.932836         0.931549        0.001783   \n",
       "2           0.929368           0.929104         0.929315        0.000105   \n",
       "\n",
       "   rank_test_score  \n",
       "0                1  \n",
       "1                1  \n",
       "2                3  "
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.DataFrame(grid_search_results.cv_results_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Random Trees\n",
    "## Mathias\n",
    "https://scikit-learn.org/stable/modules/classes.html#module-sklearn.model_selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Random Forest\n",
    "## Mathias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Neural Networks\n",
    "## Andrey"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. AUC ROC\n",
    "## Andrey"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Confusion matricies\n",
    "## Mathias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5. Which factors survived?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LogisticRegression(C=1, max_iter=1000, penalty='l1', solver='liblinear')"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "best_log_reg_lasso = LogisticRegression(penalty = 'l1', C = 1, solver = 'liblinear', max_iter = 1000)\n",
    "best_log_reg_lasso.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.        ,  4.66433991,  0.        ,  0.        ,  0.        ,\n",
       "         0.        ,  0.        , -0.45938952,  0.        , -0.0838737 ]])"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "best_log_reg_lasso.coef_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 6. To write smth"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 7. Prediction\n",
    "## Andrey"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
